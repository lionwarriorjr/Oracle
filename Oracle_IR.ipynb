{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle: Knowledge Discovery over Databases in Natural Language #\n",
    "### Johns Hopkins University ###\n",
    "### Author: Srihari Mohan ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import relevant modules\n",
    "# nltk for natural language processing tools\n",
    "# plotly for automated visualizations\n",
    "\n",
    "import os\n",
    "import config\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import itertools\n",
    "import qgrid\n",
    "import jellyfish\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import *\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.externals import joblib\n",
    "from pymining import itemmining, assocrules\n",
    "import random\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_actor(tag):\n",
    "    \n",
    "    '''Return part of speech tags that map to noun phrases in the query'''\n",
    "    \n",
    "    return tag in ['NN', 'NNP', 'NNS', 'PRP', 'CD']\n",
    "\n",
    "\n",
    "def is_desc(tag):\n",
    "    \n",
    "    '''Return part of speech tags that map to modifiers in the query'''\n",
    "    \n",
    "    return tag in ['CD', 'JJ', 'JJR', 'JJS', 'RBR', 'RBS', 'VBG']\n",
    "\n",
    "\n",
    "def is_prep(tag):\n",
    "    \n",
    "    '''Return part of speech tags that map to prepositions in the query'''\n",
    "    \n",
    "    return tag == 'IN'\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    \n",
    "    '''Return part of speech tags that map to verb phrases in the query'''\n",
    "    \n",
    "    return re.match(r'VB.*', tag[1]) is not None and tag[0] not in config.stoplist['V']\n",
    "\n",
    "\n",
    "def is_genitive(tag):\n",
    "    \n",
    "    '''Return part of speech tags that map to possessive modifiers in the query'''\n",
    "    \n",
    "    return tag[0] == '->' or tag[1] in ['POS', 'PRP$']\n",
    "\n",
    "\n",
    "def is_adv(tag):\n",
    "    \n",
    "    '''Return part of speech tags that map to adverbs in the query'''\n",
    "    \n",
    "    return tag == 'RB'\n",
    "\n",
    "\n",
    "def is_gerund(tag):\n",
    "    \n",
    "    '''Return part of speech tags that map to gerunds in the query'''\n",
    "    \n",
    "    return tag == 'VBG'\n",
    "\n",
    "\n",
    "def is_conj(tag):\n",
    "    \n",
    "    '''Return part of speech tags that map to conjunctions in the query'''\n",
    "    \n",
    "    return tag in ['and', 'or', 'not']\n",
    "\n",
    "\n",
    "def substitute_entity(subs, ent):\n",
    "    \n",
    "    '''Return if the phrase entity passed is a built-in function in dictionary subs'''\n",
    "    \n",
    "    return subs[ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize filters set to contain all keywords that are not conjunctions\n",
    "for f_key in config.keywords:\n",
    "    if config.keywords[f_key] != '->' and not is_conj(f_key):\n",
    "        config.filters.add(config.keywords[f_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display interactive data view\n",
    "qgrid.show_grid(config.X, show_toolbar=True, grid_options={'forceFitColumns': False, 'defaultColumnsWidth': 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_input(args):\n",
    "    \n",
    "    '''Prompt user to clarify ambiguous tokens in the query'''\n",
    "    \n",
    "    print('Within the context of your data, what do you mean by ' + \"\\'\" + args + \"\\'\")\n",
    "    \n",
    "    feature = input('Relevant feature: ') # prompt for feature\n",
    "    value = input('Associated value: ') # prompt for value\n",
    "    \n",
    "    return (feature, [value])\n",
    "\n",
    "\n",
    "def identify_val_via_user(relevant, value):\n",
    "    \n",
    "    '''Given a feature determined to be relevant, prompt user for its associated value if ambiguous'''\n",
    "    \n",
    "    print('Within the context of your data, how does ' \\\n",
    "          + \"\\'\" + value + \"\\'\" + 'relate to the field ' + \"\\'\" + relevant + \"\\'\")\n",
    "    val = input('Relevant value: ') # prompt for value\n",
    "    \n",
    "    return [val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sequential_entity(e_list, b_feat, actors):\n",
    "    \n",
    "    '''Perform sequence of filter reductions on the entity list'''\n",
    "    \n",
    "    response = str(' '.join([re.findall(r'^\\(?(.+[^)])\\)?', actor[0])[0] for actor in actors])).title()\n",
    "    criteria, added, expr = generate_criteria(e_list) # generate entity criteria (feature, [values])\n",
    "    \n",
    "    for i in range(len(criteria)): # sequentially filter database on entity criteria\n",
    "        \n",
    "        perform_filter('filter', criteria[i], b_feat) # perform filter \n",
    "        feat = criteria[i][0]\n",
    "        config.MOST_RECENT_QUERY.add(feat) # cache this query as most recent\n",
    "        config.ITEMSETS[len(config.ITEMSETS)-1].add(feat) # add to frequent itemsets cache\n",
    "        config.EXP_DECAY[feat] = 1 # set distance of feature since last fetched to 1\n",
    "        \n",
    "        for j in range(i, len(criteria)):\n",
    "            config.COOCCURENCE_HASH[feat][criteria[j][0]] += 1 # update cooccurrence hash\n",
    "            \n",
    "    for e in expr:\n",
    "        result, plot, status = match_expr(e, b_feat, response) # sequentially execute all user defined actions\n",
    "        \n",
    "    for feat in added: # restore original columns\n",
    "        del config.filtered[feat]\n",
    "        del config.DOMAIN_KNOWLEDGE[feat]\n",
    "        \n",
    "    config.ITEMSETS[len(config.ITEMSETS)-1] = tuple(config.ITEMSETS[len(config.ITEMSETS)-1])\n",
    "    \n",
    "    if status: # output textual response\n",
    "        \n",
    "        response = \"Oracle\\'s response: \" + str(result)\n",
    "        printmd('**\\n' + response + '**')\n",
    "        rterms = generate_features_rf_()\n",
    "        \n",
    "        if rterms:\n",
    "            printmd('**\\n' + '(Relevance Feedback) Investigate Features More Like This: ' + str(rterms) + '**\\n')\n",
    "    \n",
    "    if plot is not None:\n",
    "        return plot # return automated visualization\n",
    "\n",
    "\n",
    "def generate_criteria(e_list):\n",
    "    \n",
    "    '''Parse each entity token in the query into list of (feature, [value])'''\n",
    "    \n",
    "    entities = []\n",
    "    genitive = False\n",
    "    \n",
    "    for i in range(len(e_list)): # iterate over entity list\n",
    "        \n",
    "        if e_list[i] == config.GENITIVE:\n",
    "            genitive = True\n",
    "        elif '=>*' not in e_list[i] and not genitive:\n",
    "            entities.append(re.findall(r'^\\(?(.+[^)])\\)?', e_list[i])[0].split()) # extract each entity\n",
    "        else:\n",
    "            entities.append(re.findall(r'^\\(?(.+[^)])\\)?', e_list[i]))\n",
    "            genitive = False\n",
    "            \n",
    "    entities = list(itertools.chain.from_iterable(entities))\n",
    "    prev_feat, genitive = None, False\n",
    "    criteria, added, expr = [], [], []\n",
    "    rec_item = config.RECOMMENDATION.data[config.qry] # hash current query into RECOMMENDATIONS hash\n",
    "    conj = None\n",
    "    \n",
    "    for entity in entities:\n",
    "        \n",
    "        if is_expr(entity): # check if token is a pre-built expression\n",
    "            expr.append(entity)\n",
    "            continue\n",
    "        elif is_conj(entity): # check if token is a conjunction and tag it\n",
    "            conj = entity\n",
    "            continue\n",
    "        \n",
    "        curr_feat = match_(entity) # extract relevant feature\n",
    "        start = config.qry.find(entity)\n",
    "        end = start + len(entity)\n",
    "        rec_item.index_hash[curr_feat] = (start, end) \n",
    "        config.RECOMMENDATION.data[config.qry] = rec_item\n",
    "        \n",
    "        if curr_feat is None:\n",
    "            return user_input(entity) # prompt user to clarify input\n",
    "        else:\n",
    "            \n",
    "            val = match_arg_to_feature_value(curr_feat, entity) # extract associated value to relevant feature\n",
    "                \n",
    "            while val is None:\n",
    "                val = identify_val_via_user(curr_feat, val) # prompt user if argument specification is ambiguous\n",
    "                \n",
    "            if criteria: # handle conjunctions in query\n",
    "                if conj == 'OR': # union logic\n",
    "                    if criteria[len(criteria)-1][1] == curr_feat:\n",
    "                        criteria[len(criteria)-1][1][0] += '+' + '+'.join(val)\n",
    "                    else:\n",
    "                        pass\n",
    "                elif conj == 'NOT': # negation logic\n",
    "                    criteria.extend([(curr_feat, ['!' + v_key]) for v_key in val])\n",
    "                else:\n",
    "                    criteria.append((curr_feat, ['+'.join(val)]))\n",
    "            else: # intersection logic\n",
    "                criteria.append((curr_feat, ['+'.join(val)]))\n",
    "            conj = None\n",
    "                \n",
    "    features = [feat for feat in config.RECOMMENDATION.data[config.qry].index_hash]\n",
    "    features.sort() # sort features to specify common key\n",
    "    config.RECOMMENDATIONS[str(features)].append(config.RECOMMENDATION) # hash features in RECOMMENDATION\n",
    "    \n",
    "    return criteria, added, expr\n",
    "\n",
    "\n",
    "def is_expr(entity):\n",
    "    \n",
    "    '''Holds suite of pre built functions (averages, etc.)'''\n",
    "    \n",
    "    return entity in {'strike rate', 'on-base percentage', 'average ball velocity', 'average pitch velocity',\n",
    "                     'average x location', 'average pitch height', 'average x acceleration', 'average z acceleration'}\n",
    "\n",
    "\n",
    "def calc_score_(entity, group):\n",
    "    \n",
    "    '''Return filtered batched result after executing pre-built function '''\n",
    "    \n",
    "    if entity == 'strike rate':\n",
    "        calc = group[(group['type'] == 'S') | (group['type'] == 'X')].shape[0] / group.shape[0]\n",
    "    elif entity == 'on-base percentage':\n",
    "        calc = group[((group['event'] == 'Single') | (group['event'] == 'Double') | (group['event'] == 'Triple') | \n",
    "                      (group['event'] == 'Home Run') | (group['event'] == 'Walk') | \n",
    "                      (group['event'] == 'Field Error'))].shape[0] / group.shape[0]\n",
    "    elif entity == 'average ball velocity' or entity == 'average pitch velocity':\n",
    "        calc = group['start_speed'].sum() / group.shape[0]\n",
    "    elif entity == 'average x location':\n",
    "        calc = group['px'].sum() / group.shape[0]\n",
    "    elif entity == 'average pitch height':\n",
    "        calc = group['pz'].sum() / group.shape[0]\n",
    "    elif entity == 'average x acceleration':\n",
    "        calc = group['ax'].sum() / group.shape[0]\n",
    "    elif entity == 'average z acceleration':\n",
    "        calc = group['az'].sum() / group.shape[0]\n",
    "        \n",
    "    return calc if calc is not None else None\n",
    "\n",
    "\n",
    "def match_expr(entity, b_list, title):\n",
    "    \n",
    "    '''driver for filtering on pre-built functions'''\n",
    "    \n",
    "    plot, status = None, True\n",
    "    result, names, plot_l = '', [], []\n",
    "    \n",
    "    if b_list is not None: # if b_list is not empty, perform a plot over distribution of feature\n",
    "        \n",
    "        for name, group in config.filtered: # iterate over each group\n",
    "            calc = calc_score_(entity, group)\n",
    "            if calc is not None:\n",
    "                names.append(name) # add to x labels list\n",
    "                plot_l.append(calc) # add to y values list\n",
    "                result += (name + ': ' + str(calc) + '\\n')\n",
    "        \n",
    "        if plot_l: # if calculated results are returned, generate the plot\n",
    "            plot = automate_plot_by_(names, plot_l, entity, title, 'bar') # call plotting subroutine\n",
    "            status = False # set time_series plot status to False\n",
    "    \n",
    "    else: # otherwise plot a timeseries by default\n",
    "        result = { # evaluate expression over filtered dataset and store result\n",
    "            'strike rate': lambda Z: str(Z[(Z['type'] == 'S') | (Z['type'] == 'X')].shape[0] / Z.shape[0]) if Z.shape[0] > 0 else 0,\n",
    "            'on-base percentage': lambda Z: str(Z[((Z['event'] == 'Single') | (Z['event'] == 'Double') | (Z['event'] == 'Triple') | \n",
    "                                                   (Z['event'] == 'Home Run') | (Z['event'] == 'Walk') | \n",
    "                                                   (Z['event'] == 'Field Error'))].shape[0] / Z.shape[0]) if Z.shape[0] > 0 else 0,\n",
    "            'average ball velocity': lambda Z: str(Z['start_speed'].sum() / Z.shape[0]) if Z.shape[0] > 0 else 0,\n",
    "            'average pitch velocity': lambda Z: str(Z['start_speed'].sum() / Z.shape[0]) if Z.shape[0] > 0 else 0,\n",
    "            'average x location': lambda Z: str(Z['px'].sum() / Z.shape[0]) if Z.shape[0] > 0 else 0,\n",
    "            'average pitch height': lambda Z: str(Z['pz'].sum() / Z.shape[0]) if Z.shape[0] > 0 else 0,\n",
    "            'average x acceleration': lambda Z: str(Z['ax'].sum() / Z.shape[0]) if Z.shape[0] > 0 else 0,\n",
    "            'average z acceleration': lambda Z: str(Z['az'].sum() / Z.shape[0]) if Z.shape[0] > 0 else 0\n",
    "        }[entity](config.filtered)\n",
    "        \n",
    "        time_series = config.filtered.groupby('Date') # group filtered dataset by date\n",
    "        \n",
    "        for name, group in time_series:\n",
    "            calc = calc_score_(entity, group) # calculate result\n",
    "            if calc is not None:\n",
    "                names.append(name)\n",
    "                plot_l.append(calc)\n",
    "                \n",
    "        if plot_l:\n",
    "            plot = automate_plot_by_(names, plot_l, entity, title, 'scatter') # plot time series chart\n",
    "            \n",
    "    return result, plot, status\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    \n",
    "    '''Return if a string can be parsed as a number'''\n",
    "    \n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sequential_filter(f_list):\n",
    "    \n",
    "    '''Execute sequential filters over dataset on supplied filtering criteria'''\n",
    "    \n",
    "    config.MOST_RECENT_QUERY = set() # reinitialize MOST_RECENT_QUERY\n",
    "    config.ITEMSETS.append(set()) # reinitialize ITEMSETS\n",
    "    \n",
    "    for feat in config.EXP_DECAY:\n",
    "        config.EXP_DECAY[feat] += 1 # increment all features distance from being last fetched by 1\n",
    "        \n",
    "    config.filtered = config.X.copy() # reinitialize filtered\n",
    "    filters = feature_assoc_filters_helper(f_list) # generate filtering criteria as list of (feature, [values])\n",
    "    grouped, b_list = None, []\n",
    "    \n",
    "    for i in range(len(filters)): # iterate over each filtering criterion\n",
    "        \n",
    "        f_, c_ = filters[i][0], filters[i][1] # extract filtering operation, criteria\n",
    "        feat = c_[0]\n",
    "        config.MOST_RECENT_QUERY.add(feat)\n",
    "        config.ITEMSETS[len(config.ITEMSETS)-1].add(feat) # update ITEMSETS\n",
    "        config.EXP_DECAY[feat] = 1 # set distance since being fetched to 1\n",
    "        \n",
    "        for j in range(i, len(filters)):\n",
    "            config.COOCCURENCE_HASH[feat][filters[j][1][0]] += 1 # update cooccurence hash\n",
    "            \n",
    "        if not c_[1]:\n",
    "            b_list.append(feat) # add to b_list if filtering criteria is over a feature's entire distribution\n",
    "            continue\n",
    "            \n",
    "        perform_filter(f_, c_, False) # perform appropriate filter\n",
    "        \n",
    "    if len(b_list) > 0:\n",
    "        grouped, b_feat = by_([b_list, []])\n",
    "        \n",
    "    return (config.filtered, None) if grouped is None else (grouped, b_feat)\n",
    "\n",
    "\n",
    "def feature_assoc_filters_helper(filters):\n",
    "    \n",
    "    '''Return list of (relevant feature, [associated values]) tuples to filter automatically'''\n",
    "    \n",
    "    f_ = [] # initilize result list\n",
    "    config.RECOMMENDATION = config.Recommendation(config.qry) # initialize RECOMMENDATION to hash on current query\n",
    "    rec_item = config.RecommendationItem()\n",
    "    prev_numeric = None\n",
    "    \n",
    "    for f_item in filters:\n",
    "        \n",
    "        tokens = re.findall(r'\\(.+?\\)|AND|OR|NOT', f_item) # tokenize filters\n",
    "        tokens[0] = re.findall(r'%(.+)%', tokens[0])[0]\n",
    "        tokens[1] = tokens[1][1:]\n",
    "        tokens = [re.findall(r'\\((.+?)\\)', token)[0] if '(' in token else token for token in tokens]\n",
    "        prev_feat, relevant, val, cat = None, None, None, ''\n",
    "        i, inc, c_flag = 1, False, False\n",
    "        \n",
    "        while i < len(tokens): # iterate over each filter in the filtered list\n",
    "            \n",
    "            if prev_numeric is not None:\n",
    "                relevant = prev_numeric\n",
    "                val = [tokens[i]] # set argument value to match token if a numeric feature\n",
    "                \n",
    "            if relevant is None:\n",
    "                \n",
    "                relevant = match_(tokens[i]) # extract relevant feature for tokens[i]\n",
    "                if relevant is None:\n",
    "                    relevant, val = user_input(tokens[i]) # prompt user if ambiguous\n",
    "                start = config.qry.find(tokens[i]) # start index stored before cached in RECOMMENDATION\n",
    "                end = start + len(tokens[i]) # end index stored before cached in RECOMMENDATION\n",
    "                rec_item.index_hash[relevant] = (start, end) \n",
    "                \n",
    "                # set current recommendation item as the value to RECOMMENDATION hashsed on the current query\n",
    "                config.RECOMMENDATION.data[config.qry] = rec_item \n",
    "                \n",
    "            if val is None:\n",
    "                \n",
    "                cat += tokens[i]\n",
    "                \n",
    "                while (i+1) < len(tokens) and is_conj(tokens[i+1].lower()): # check if next token is a conjunction\n",
    "                    \n",
    "                    if (i+2) < len(tokens):\n",
    "                        \n",
    "                        inc = True\n",
    "                        next_feat = match_(tokens[i+2]) # extract relevant feature for this next token\n",
    "                        \n",
    "                        if next_feat is None:\n",
    "                            next_feat, _ = user_input(tokens[i+2]) # if ambiguous prompt the user\n",
    "                        \n",
    "                        if relevant != next_feat: # check if the current parsed feature matches the next feature\n",
    "                            relevant = next_feat # if not append to the result list sequentially\n",
    "                            i += 2\n",
    "                            break\n",
    "                        \n",
    "                        else:\n",
    "                            \n",
    "                            # otherwise append the next feature value to the running result\n",
    "                            cat += tokens[i+1] + tokens[i+2] \n",
    "                            i += 3\n",
    "                            \n",
    "                for core_entity in config.CORE:\n",
    "                    if jellyfish.jaro_distance(core_entity, cat) > config.NAME_THRESHOLD:\n",
    "                        c_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if not c_flag:\n",
    "                \n",
    "                if val is None:\n",
    "                    \n",
    "                    # call subroutine to generate filter criteria for current (feature, [arguments]) pair\n",
    "                    result = generate_filter_criteria(cat) \n",
    "                    \n",
    "                    if not result[1] or result[1][0] != 'is.numeric':\n",
    "                        f_.append((tokens[0], result)) # append to result list\n",
    "                        prev_numeric = None\n",
    "                    else:\n",
    "                        prev_numeric = result[0]\n",
    "                else:\n",
    "                    f_.append((tokens[0], (relevant, val))) # append to result list\n",
    "                    prev_numeric = None\n",
    "                    \n",
    "            if not inc: \n",
    "                i += 1\n",
    "                \n",
    "            inc, c_flag, val, cat = False, False, None, ''\n",
    "            \n",
    "    return f_\n",
    "        \n",
    "\n",
    "def perform_filter(f_key, c_key, b_list):\n",
    "    \n",
    "    '''Perform relevant filter'''\n",
    "    \n",
    "    result = {\n",
    "        'filter': lambda D: filter_(c_key, b_list),\n",
    "        'by': lambda D: by_(c_key),\n",
    "        'over': lambda D: over_(c_key),\n",
    "        'under': lambda D: under_(c_key),\n",
    "        'between': lambda D: between_(c_key),\n",
    "        'except': lambda D: except_(c_key),\n",
    "        'near': lambda D: near_(c_key),\n",
    "        'until': lambda D: until_(c_key),\n",
    "        'to': lambda D: to_(c_key),\n",
    "        'after': lambda D: after_(c_key),\n",
    "        'before': lambda D: before_(c_key),\n",
    "        'against': lambda D: compare_(c_key)\n",
    "    }[f_key](config.filtered)\n",
    "    \n",
    "\n",
    "def generate_filter_criteria(args, hint=None):\n",
    "    \n",
    "    '''Return relevant (feature, [values]) tuple that matches argument'''\n",
    "    \n",
    "    criteria = []\n",
    "    \n",
    "    for identifier in config.IDENTIFIERS:\n",
    "        for name in config.IDENTIFIERS[identifier]:\n",
    "            if jellyfish.jaro_distance(args, name) > config.NAME_THRESHOLD:\n",
    "                return (identifier, name)\n",
    "            \n",
    "    c_keys = ['AND', 'OR', 'NOT']\n",
    "    pat = '((?:' + '|'.join(c_keys) + '))'\n",
    "    c_list = re.split(pat, args) # split on conjunctions\n",
    "    \n",
    "    if c_list:\n",
    "        relevant = match_(c_list[0]) # extract relevant feature\n",
    "        \n",
    "    if not c_list or relevant is None:\n",
    "        return user_input(args) # if ambiguous, prompt user to specify\n",
    "    \n",
    "    print('\\nfeature association: most relevant feature is ' + relevant)\n",
    "    \n",
    "    if 'is.numeric' in list(config.DOMAIN_KNOWLEDGE[relevant].values())[0]:\n",
    "        return (relevant, ['is.numeric']) # tag feature with 'is.numeric' if it is numeric\n",
    "    elif args in config.DOMAIN_KNOWLEDGE[relevant]:\n",
    "        return (relevant, [])\n",
    "    \n",
    "    conj = None\n",
    "    \n",
    "    for term in c_list:\n",
    "        \n",
    "        if is_conj(term.lower()):\n",
    "            conj = term\n",
    "        else:\n",
    "            lookup = re.match('(.+)', term).group() # extract token\n",
    "            val = match_arg_to_feature_value(relevant, lookup) # look up associated filter criteria on feature\n",
    "            print('generate_criteria: ' + str(val))\n",
    "            \n",
    "            while val is None:\n",
    "                val = identify_val_via_user(relevant, val) # prompt user if associated value is ambiguous\n",
    "                \n",
    "            if criteria:\n",
    "                if conj == 'OR' or len(val) > 1: # handle union logic\n",
    "                    criteria[len(criteria)-1] += '+' + '+'.join(val)\n",
    "                elif conj == 'NOT': # handle negation logic\n",
    "                    criteria.extend(['!' + v_key for v_key in val])\n",
    "                else:\n",
    "                    criteria.append('+'.join(val))\n",
    "            else: # handle intersection logic\n",
    "                criteria.append('+'.join(val))\n",
    "                \n",
    "            conj = None\n",
    "            \n",
    "    return (relevant, criteria)\n",
    "    \n",
    "\n",
    "def filter_(c_key, b_list):\n",
    "    \n",
    "    '''Perform filter on categorical feature'''\n",
    "    \n",
    "    feature = c_key[0] # extract feature\n",
    "    args = c_key[1] # extract relevant values of feature to filter on\n",
    "    print('(' + str(feature) + ', ' + str(args) + '): filter_')\n",
    "    \n",
    "    if b_list: # handle automated filtering over distribution of a feature\n",
    "        for index, f_tok in enumerate(args): # iterate over each argument\n",
    "            \n",
    "            f_ = re.split(r'(?:\\+|!)', f_tok) # split by filters handled with conjunctive logic\n",
    "            \n",
    "            if '!' in f_tok: # handle negation\n",
    "                exec_str = \"config.filtered = config.filtered.apply(lambda g: (g[g[\\'\" + feature + \"\\'] != \\'\" + f_[1] + \"\\']))\"\n",
    "                print(exec_str) # generate string to execute code that automates filtering step\n",
    "                exec(exec_str) # execute code\n",
    "            else:\n",
    "                union = [item for item in f_]\n",
    "                exec_str = \"config.filtered = config.filtered.apply(lambda g: (g[g[\\'\" + feature + \"\\'].isin(\" + str(union) + \")]))\"\n",
    "                print(exec_str) # generate string to execute code that automates filtering step\n",
    "                exec(exec_str) # execute code\n",
    "                \n",
    "            # filter over each value of a categorical feature\n",
    "            config.filtered = config.filtered.groupby(b_list) # group by features listed in b_list\n",
    "    else:\n",
    "        exec_str = 'config.filtered['\n",
    "        \n",
    "        for index, f_tok in enumerate(args): # iterate over each argument\n",
    "            \n",
    "            f_ = re.split(r'(?:\\+|!)', f_tok) # split on conjunctions\n",
    "            \n",
    "            if '!' in f_tok: # negation logic\n",
    "                exec_str = \"config.filtered = config.filtered[(config.filtered[\\'\" + feature + \"\\'] != \\'\" + f_[1] + \"\\')\"\n",
    "                print('\\n' + exec_str)\n",
    "                exec(exec_str) # execute automated filter\n",
    "                continue\n",
    "                \n",
    "            exec_str = \"config.filtered[(config.filtered[\\'\" + feature + \"\\'] == \\'\" + f_[0] + \"\\')\"\n",
    "            \n",
    "            for i in range(1, len(f_)):\n",
    "                exec_str += \" | (config.filtered[\\'\" + feature + \"\\'] == \\'\" + f_[i] + \"\\')\" # handle union logic\n",
    "                \n",
    "            exec_str += ']'\n",
    "            exec_str = \"config.filtered = \" + exec_str\n",
    "            print('\\n' + exec_str)\n",
    "            exec(exec_str) # execute automated filter\n",
    "    \n",
    "    # return filtered result\n",
    "    return config.filtered\n",
    "\n",
    "\n",
    "def by_(c_key):\n",
    "    \n",
    "    '''Filter database over distribution of a feature'''\n",
    "    \n",
    "    features = c_key[0] # extract features\n",
    "    config.filtered = config.X.groupby(features) # group by features\n",
    "    \n",
    "    return config.filtered, features\n",
    "\n",
    "\n",
    "def over_(c_key):\n",
    "    \n",
    "    '''Filter database on values over a threshold for a feature'''\n",
    "    \n",
    "    feature = c_key[0] # extract feature\n",
    "    args = c_key[1] # extract value to filter on\n",
    "    exec_str = \"config.filtered = config.filtered[(config.filtered[\\'\" + feature + \"\\'] > \" + args[0] + \")]\"\n",
    "    print('\\n' + exec_str) \n",
    "    exec(exec_str) # execute automated filter\n",
    "    \n",
    "    return config.filtered\n",
    "\n",
    "\n",
    "def under_(c_key):\n",
    "    \n",
    "    '''Filter database on values under a threshold for a feature'''\n",
    "    \n",
    "    feature = c_key[0] # extract feature\n",
    "    args = c_key[1] # extract value to filter on\n",
    "    exec_str = \"config.filtered = config.filtered[(config.filtered[\\'\" + feature + \"\\'] < \" + args[0] + \")]\"\n",
    "    print('\\n' + exec_str)\n",
    "    exec(exec_str) # execute automated filter\n",
    "    \n",
    "    return config.filtered\n",
    "\n",
    "\n",
    "def between_(c_key):\n",
    "    \n",
    "    '''Filter database on values between two thresholds for a feature'''\n",
    "    \n",
    "    feature = c_key[0] # extract feature\n",
    "    args = c_key[1] # extract value to filter on\n",
    "    left, right = args[0], args[1]\n",
    "    exec_str = (\"config.filtered = config.filtered[(config.filtered[\\'\" + feature + \"\\'] > \" + left + \")\"\n",
    "                \"& (config.filtered[\\'\" + feature + \"\\'] < \" + right + \")]\")\n",
    "    print('\\n' + exec_str)\n",
    "    exec(exec_str) # execute automated filter\n",
    "    \n",
    "    return config.filtered\n",
    "\n",
    "\n",
    "def except_(c_key):\n",
    "    \n",
    "    '''Filter database on values of a categorical feature except for those specified in the argument'''\n",
    "    \n",
    "    feature = c_key[0] # extract feature\n",
    "    args = c_key[1] # extract values to filter on\n",
    "    unique_vals = list(set(list(config.filtered[feature].unique())) - set([args]))\n",
    "    unique_vals = [x for x in unique_vals if x == x]\n",
    "    unique_vals = '+'.join(unique_vals)\n",
    "    config.filtered = filter_([feature, unique_vals], None, config.filtered) # perform filter\n",
    "    \n",
    "    return config.filtered\n",
    "\n",
    "\n",
    "def near_(c_key):\n",
    "    \n",
    "    '''Filter database on values within +/- 0.5 std of the argument on a numerical feature'''\n",
    "    \n",
    "    feature = c_key[0] # extract relevant feature\n",
    "    args = c_key[1] # extract values to filter on\n",
    "    left = str(float(args[0]) - 0.5 * config.filtered[feature].std()) # left bound -0.5 std\n",
    "    right = str(float(args[1]) + 0.5 * config.filtered[feature].std()) # right bound +0.5 std\n",
    "    \n",
    "    if left > right:\n",
    "        left, right = right, left\n",
    "        \n",
    "    config.filtered = between_([feature, [left,right]]) # perform filter\n",
    "    \n",
    "    return config.filtered\n",
    "\n",
    "\n",
    "def automate_plot_by_(x, y, entity, title, chart_type):\n",
    "    \n",
    "    '''Return automated visualization relevant to queried features'''\n",
    "    \n",
    "    font=dict(family='Courier New, monospace', size=24, color='#7f7f7f') # set font\n",
    "    \n",
    "    xaxis=dict( # set x-axis plot attributes\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    yaxis=dict( # set y-axis plot attributes\n",
    "        title = entity,\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if chart_type == 'bar':\n",
    "        data = [Bar(x=x, y=y)] # set data for bar chart chart\n",
    "    else:\n",
    "        data = [plotly.graph_objs.Scatter(x=x, y=y)] # set data for defaulted time series scatter\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label='1m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=6,\n",
    "                     label='6m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        xaxis['rangeselector'] = rangeselector # add range selector\n",
    "        xaxis['rangeslider'] = dict()\n",
    "        \n",
    "    layout = plotly.graph_objs.Layout( # adjust plot layout\n",
    "        title=title,\n",
    "        font=font,\n",
    "        xaxis=xaxis,\n",
    "        yaxis=yaxis\n",
    "    )\n",
    "    \n",
    "    fig = plotly.graph_objs.Figure(data=data, layout=layout) # store automated plot\n",
    "    \n",
    "    return py.iplot(fig, filename='extract_bar') if chart_type == 'bar' else py.iplot(fig, filename='extract_scatter')        \n",
    "    \n",
    "\n",
    "def match_(args):\n",
    "    \n",
    "    '''Return relevant feature to filter on'''\n",
    "    \n",
    "    max_conf, max_feat = 0, ''\n",
    "    \n",
    "    for entry in config.DOMAIN_KNOWLEDGE: # iterate over terms in the system's domain knowledge\n",
    "        \n",
    "        if args in config.DOMAIN_KNOWLEDGE[entry]: # if term matches exactly return it\n",
    "            return entry\n",
    "        \n",
    "        tokens = args.split() # otherwise split tokens and accumulate evidence of belonging to each feature\n",
    "        conf_f = 0 # confidence\n",
    "        \n",
    "        for token in tokens: # iterate over each token\n",
    "            \n",
    "            curr = 0\n",
    "            \n",
    "            for desc in list(config.DOMAIN_KNOWLEDGE[entry].keys()): # iterate over each term in domain knowledge\n",
    "                \n",
    "                # compute string similarity of query vs term in DOMAIN_KNOWLEDGE\n",
    "                curr = max(jellyfish.jaro_distance(desc, token), curr) # string similarity by jaro_distance\n",
    "            \n",
    "            conf_f += curr # accumulate confidence score for feture\n",
    "            \n",
    "        conf_f /= len(tokens) # scaled confidence level of feature match\n",
    "        \n",
    "        if conf_f > max_conf: # update max condfidence level and associated feature\n",
    "            max_conf, max_feat = conf_f, entry\n",
    "    \n",
    "    if max_conf > config.CONF_THRESHOLD:\n",
    "        return max_feat # return result\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def match_arg_to_feature_value(feature, args):\n",
    "    \n",
    "    '''Return list of relevant arguments that match feature value'''\n",
    "    \n",
    "    print('(' + feature + ', ' + args + '): ' + 'match_arg_to_feature_value')\n",
    "    \n",
    "    if 'is.numeric' in list(config.DOMAIN_KNOWLEDGE[feature].values())[0]:\n",
    "        if not is_number(args):\n",
    "            return None\n",
    "        return [args] # return argument if relevant feature is tagged as being numeric\n",
    "    \n",
    "    unique_vals = list(set(list(itertools.chain.from_iterable(config.DOMAIN_KNOWLEDGE[feature].values()))))\n",
    "    vals = {key:0 for key in unique_vals} # initialize hash to accumulate evidence for each value of feature\n",
    "    tokens = args.split() # tokenize argument\n",
    "    max_val, max_arg = 0, []\n",
    "    \n",
    "    for token in tokens: # iterate over each token\n",
    "        \n",
    "        for lookup in config.DOMAIN_KNOWLEDGE[feature]: # lookup relevant modifiers in domain knowledge\n",
    "            \n",
    "            jaro = jellyfish.jaro_distance(token, lookup) # compute string similarity of modifier vs query token\n",
    "            \n",
    "            # jaro normalized as a confidence between [0, 1]\n",
    "            if jaro > config.CONF_THRESHOLD: # check if confidence is greater than preset threshold\n",
    "                \n",
    "                # iterate over list of feature values that match the current modifier in domain knowledge\n",
    "                for i in range(len(config.DOMAIN_KNOWLEDGE[feature][lookup])):\n",
    "                    \n",
    "                    vals[config.DOMAIN_KNOWLEDGE[feature][lookup][i]] += jaro # accumulate evidence for lookup in hash\n",
    "                    \n",
    "                    if vals[config.DOMAIN_KNOWLEDGE[feature][lookup][i]] > max_val:\n",
    "                        \n",
    "                        max_val = vals[config.DOMAIN_KNOWLEDGE[feature][lookup][i]] # update max confidence\n",
    "                        max_arg = [config.DOMAIN_KNOWLEDGE[feature][lookup][i]] # update associated max argument\n",
    "                    \n",
    "                    elif vals[config.DOMAIN_KNOWLEDGE[feature][lookup][i]] == max_val:\n",
    "                        \n",
    "                        # accomodate for series of feature values that match the current modifier equally \n",
    "                        max_arg.append(config.DOMAIN_KNOWLEDGE[feature][lookup][i])\n",
    "                        \n",
    "    return max_arg if max_arg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_features_rf_():\n",
    "    \n",
    "    '''Generate Bag-of-words of Relevant Features on Most Recent Query\n",
    "    \n",
    "    Current implementation supports suggestion of relevant features by relevance feedback (Rochio algorithm)\n",
    "    Current implementation also supports suggestion of relevant features by frequent itemsets\n",
    "    Defaulted to implementation of relevance feedback, with results cached for future reference\n",
    "    \n",
    "    Oracle caches features fetched over time and recalculates their weights in the cooccurence hash by\n",
    "    time since last hit using an exponential decay\n",
    "    '''\n",
    "    \n",
    "    qry_vector = defaultdict(float)\n",
    "    rterms, nrterms = [], []\n",
    "    \n",
    "    # cooccurence_hash => 'document vector'\n",
    "    # represents (term, features) matrix where weight for each (term, feature) is dependent on cooccurence strength\n",
    "    cooccurence_hash = config.COOCCURENCE_HASH.copy()\n",
    "    \n",
    "    for term, steps in config.EXP_DECAY.items():\n",
    "        cooccurence_hash[term][term] *= config.DECAY * (np.e ** (-config.DECAY * steps)) # update weights by exp decay\n",
    "        \n",
    "    for feat, term_wgts in cooccurence_hash.items(): # iterate over feature, weight pairs in cooccurence hash\n",
    "        \n",
    "        if feat in config.MOST_RECENT_QUERY: # construct query vector on terms in most recent query\n",
    "            qry_vector[feat] += 1\n",
    "            rterms.append(term_wgts)\n",
    "        else:\n",
    "            nrterms.append(term_wgts) # construct list of nonrelevant terms for Rochio\n",
    "            \n",
    "    reform = rochio_algo(qry_vector, rterms, nrterms, 1, 0.75, 0.15) # compute reformulated query vector by Rochio\n",
    "    rterms = [] # reinitialize rterms to hold suggested terms to investigate\n",
    "    \n",
    "    for term in cooccurence_hash: # iterate over each term (key) in cooccurence hash\n",
    "        cos_sim = cosine_sim(reform, cooccurence_hash[term]) # compute similarity of each vector in cooccurence_hash\n",
    "        rterms.append((term, cos_sim)) # append to rterms\n",
    "        \n",
    "    rterms.sort(key=lambda x: x[1]) # sort rterms before inserting into cache\n",
    "    \n",
    "    # only include features deemed relevant over a preset threshold\n",
    "    rterms = [term[0] for term in rterms if term[1] > config.RELEVANCE_FEEDBACK_THRESHOLD]\n",
    "    \n",
    "    return rterms\n",
    "\n",
    "def generate_queries_itemsets():\n",
    "    \n",
    "    '''Return association rules from frequent itemsets analysis of relevant features to investigate'''\n",
    "    \n",
    "    relim_input = itemmining.get_relim_input(config.ITEMSETS)\n",
    "    item_sets = itemmining.relim(relim_input, min_support=config.ASSOC_MIN_SUPPORT) # generate frequent itemsets\n",
    "    rules = assocrules.mine_assoc_rules(item_sets, min_support=config.ASSOC_MIN_SUPPORT, \n",
    "                                        min_confidence=config.ASSOC_MIN_CONFIDENCE) # generate association rules\n",
    "    rules.sort(key=lambda x: -1 * x[2] * x[3]) # sort rules before inserting into cache\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rochio_algo(qry_vector, rel_terms, nonrel_terms, a, B, y):\n",
    "    \n",
    "    '''Relevance Feedback by Rochio Algorithm for Automated Query Suggestion\n",
    "    \n",
    "    Extension of original algorithm by caching results for future reference\n",
    "    '''\n",
    "    \n",
    "    rels, nonrels = defaultdict(float), defaultdict(float)\n",
    "    dr, dnr = len(rel_terms), len(nonrel_terms)\n",
    "    \n",
    "    for rel_term in rel_terms: # iterate over relevant feature set\n",
    "        \n",
    "        if dr <= 0: break\n",
    "            \n",
    "        for rel_key in rel_term: # iterate over each (feature, weight) tuple in rels\n",
    "            rels[rel_key] += (B / dr) * rel_term[rel_key] # reweight each feature weight in relevant set\n",
    "            \n",
    "    for nonrel_term in nonrel_terms: # iterate over nonrelevant feature set\n",
    "        \n",
    "        if dnr <= 0: break\n",
    "            \n",
    "        for nonrel_key in nonrel_term: # iterate over each (feature, weight) tuple in nonrels\n",
    "            nonrels[nonrel_key] += (y / dnr) * nonrel_term[nonrel_key] # reweight each feature weight in nonrelevant set \n",
    "            \n",
    "    for term in qry_vector:\n",
    "        qry_vector[term] *= a # rewight initial query vector by alpha\n",
    "    \n",
    "    reform = defaultdict(float) # initialize reformulated query vector\n",
    "    \n",
    "    for text in [qry_vector, rels]:\n",
    "        for term in text:\n",
    "            reform[term] += text[term]\n",
    "            \n",
    "    for nonrel_key in nonrels:\n",
    "        \n",
    "        if nonrel_key in reform:\n",
    "            \n",
    "            reform[nonrel_key] -= nonrels[nonrel_key] # offset reformulated query to drift from the nonrelevant set\n",
    "            \n",
    "            if reform[nonrel_key] < 0:\n",
    "                reform.pop(nonrel_key, None) # reset features with weights < 0 to 0 in reform\n",
    "    \n",
    "    # return reformulated query vector\n",
    "    return reform\n",
    "\n",
    "def cosine_sim(vec1, vec2, vec1_norm = 0.0, vec2_norm = 0.0):\n",
    "    \n",
    "    '''Return cosine similarity between two vectors'''\n",
    "    \n",
    "    if not vec1_norm:\n",
    "        vec1_norm = sum(v * v for v in vec1.values())\n",
    "    if not vec2_norm:\n",
    "        vec2_norm = sum(v * v for v in vec2.values())\n",
    "\n",
    "    # save some time of iterating over the shorter vec\n",
    "    if len(vec1) > len(vec2):\n",
    "        vec1, vec2 = vec2, vec1\n",
    "\n",
    "    # calculate the inner product\n",
    "    inner_product = sum(vec1.get(term, 0) * vec2.get(term, 0) for term in vec1.keys())\n",
    "\n",
    "    return inner_product / np.sqrt(vec1_norm * vec2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query your data: what is the on-base percentage of lefty batters against oriole pitchers on fastballs or curveballs where the start velocity of the pitch is over 90, the horizontal movement of the pitch is under 12, height is over 2\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# request query from user\n",
    "config.qry = input('Query your data: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('on-base', 'JJ'),\n",
       " ('percentage', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('lefty', 'JJ'),\n",
       " ('batters', 'NNS'),\n",
       " ('against', 'IN'),\n",
       " ('oriole', 'JJ'),\n",
       " ('pitchers', 'NNS'),\n",
       " ('on', 'IN'),\n",
       " ('fastballs', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('curveballs', 'NNS'),\n",
       " ('where', 'WRB'),\n",
       " ('the', 'DT'),\n",
       " ('start', 'NN'),\n",
       " ('velocity', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('pitch', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('90', 'CD'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('horizontal', 'JJ'),\n",
       " ('movement', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('pitch', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('under', 'IN'),\n",
       " ('12', 'CD'),\n",
       " (',', ','),\n",
       " ('height', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('2', 'CD')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(config.qry) # tokenize user query\n",
    "pos_tag = nltk.pos_tag(tokens) # apply part of speech tagger\n",
    "pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['percentage', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['batters', 'NNS'],\n",
       " ['against', 'IN'],\n",
       " ['pitchers', 'NNS'],\n",
       " ['on', 'IN'],\n",
       " ['fastballs', 'NNS'],\n",
       " ['or', 'CC'],\n",
       " ['curveballs', 'NNS'],\n",
       " ['where', 'WRB'],\n",
       " ['start velocity', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['pitch', 'NN'],\n",
       " ['over', 'IN'],\n",
       " ['90', 'CD'],\n",
       " ['movement', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['pitch', 'NN'],\n",
       " ['under', 'IN'],\n",
       " ['12', 'CD'],\n",
       " ['height', 'NN'],\n",
       " ['over', 'IN'],\n",
       " ['2', 'CD']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actors, ind, verbs = [], [], []\n",
    "prev = False\n",
    "\n",
    "for i, tag in enumerate(pos_tag): # enumerate over each element of the tagged list\n",
    "    \n",
    "    tag = list(tag)\n",
    "    \n",
    "    if is_prep(tag[1]) or pos_tag[i][0] in config.keywords: # check if token is a preposition or a keyword\n",
    "        actors.append(tag)\n",
    "        prev = False    \n",
    "    elif is_genitive(tag) or is_verb(tag) or is_actor(tag[1]):\n",
    "        \n",
    "        if is_genitive(tag) or is_verb(tag): # check if tagged element is a possessive modifier or a verb phrase\n",
    "            \n",
    "            if is_genitive(tag): \n",
    "                tag[0] = config.GENITIVE # reset query token to be the genitive placeholder '->'\n",
    "            elif is_verb(tag):\n",
    "                verbs.append(i) # add to verbs list\n",
    "                \n",
    "            actors.append(tag) # add to actors list\n",
    "            prev = False\n",
    "        else:\n",
    "            \n",
    "            if not prev:\n",
    "                actors.append(tag)\n",
    "                ind.append(i)\n",
    "            else:\n",
    "                actors[len(actors)-1][0] += ' ' + tag[0] # concatenate noun phrases with adjacent NNP\n",
    "            \n",
    "            prev = True\n",
    "    else:\n",
    "        prev = False\n",
    "        \n",
    "actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['(on-base percentage)', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['(lefty batters)', 'NNS'],\n",
       " ['against', 'IN'],\n",
       " ['(oriole pitchers)', 'NNS'],\n",
       " ['on', 'IN'],\n",
       " ['(fastballs)', 'NNS'],\n",
       " ['or', 'CC'],\n",
       " ['(curveballs)', 'NNS'],\n",
       " ['where', 'WRB'],\n",
       " ['(start velocity)', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['(pitch)', 'NN'],\n",
       " ['over', 'IN'],\n",
       " ['(90)', 'CD'],\n",
       " ['(horizontal movement)', 'NN'],\n",
       " ['of', 'IN'],\n",
       " ['(pitch)', 'NN'],\n",
       " ['under', 'IN'],\n",
       " ['(12)', 'CD'],\n",
       " ['(height)', 'NN'],\n",
       " ['over', 'IN'],\n",
       " ['(2)', 'CD']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ind, v_ind = 0, 0 # initialize actor index and verb index\n",
    "prev = False\n",
    "\n",
    "for i in range(len(ind)):\n",
    "    \n",
    "    if a_ind < len(actors) and v_ind < len(verbs) and is_verb(actors[a_ind]):\n",
    "        \n",
    "        index = verbs[v_ind] + 1\n",
    "        \n",
    "        while index < len(pos_tag) and is_adv(pos_tag[index][1]):\n",
    "            actors[a_ind][0] += ' ' + pos_tag[index][0] # group adverbs and verb phrases as a single entity\n",
    "            index += 1\n",
    "            \n",
    "        v_ind += 1\n",
    "        \n",
    "    while a_ind < len(actors) and not is_actor(actors[a_ind][1]):\n",
    "        a_ind += 1\n",
    "    \n",
    "    index = ind[i]-1\n",
    "    \n",
    "    while index >= 0 and is_desc(pos_tag[index][1]):\n",
    "        \n",
    "        # concatenate noun phrases with adjacent modifiers\n",
    "        actors[a_ind][0] = pos_tag[index][0] + ' ' + actors[a_ind][0]\n",
    "        index -= 1\n",
    "        \n",
    "    a_ind += 1\n",
    "    \n",
    "for i, tag in enumerate(actors):\n",
    "    if is_actor(tag[1]):\n",
    "        tag[0] = '(' + tag[0] + ')' # wrap noun phrases in parenthesis for tagging\n",
    "\n",
    "# remove gerund and noun phrase modifers adjacent to the concatenated sets contructed above\n",
    "actors[:] = [actors[i] for i in range(len(actors)) if not ((i+1) < len(actors) and \n",
    "                                                       (is_gerund(actors[i][1]) and \n",
    "                                                       is_actor(actors[i+1][1])))]        \n",
    "actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(lefty batters)->(on-base percentage)=>*(%filter%)((oriole pitchers))=>*(%filter%)((fastballs)OR(curveballs))=>*(%filter%)((start velocity))=>*(%over%)((90))=>*(%filter%)((pitch))=>*(%filter%)((horizontal movement))=>*(%under%)((12))=>*(%filter%)((pitch))=>*(%filter%)((height))=>*(%over%)((2))'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_p, prev_a, prev_f, flag = False, False, False, -1 # set preposition, actor, filter flags\n",
    "open_v = False # set flag to check if currently parsing a verb phrase\n",
    "extag = '' # parsed result\n",
    "pos_index = 0 # holds current position to edit result at in the finite state transduction\n",
    "\n",
    "for i in range(len(actors)):\n",
    "    \n",
    "    if flag == 0:\n",
    "        \n",
    "        if actors[i][0].lower() != 'of':\n",
    "            extag += '=>*(%filter%)(' + actors[i][0]\n",
    "            flag, open_v = 1, True\n",
    "        continue\n",
    "        \n",
    "    if is_verb(actors[i]): # check if token is a verb phrase\n",
    "        \n",
    "        op = config.keywords[actors[i][0]] # lookup relevant keyword\n",
    "        \n",
    "        if not op: # ckeck if the token matches a preset token in keywords\n",
    "            \n",
    "            # substitute op with user specified action\n",
    "            op = '=>*(%' + actors[i][0].lower() + '%)'\n",
    "            \n",
    "        pos_index = len(extag)\n",
    "        extag += op + '(' \n",
    "        open_v = True # set current parsing of verb phrase to true\n",
    "        \n",
    "    elif is_conj(actors[i][0].lower()): # check if token is a conjunction\n",
    "        \n",
    "        op = config.keywords[actors[i][0].lower()] # lookup relevant keyword\n",
    "        pos_index = len(extag)\n",
    "        extag += op # append to result\n",
    "        continue\n",
    "        \n",
    "    elif is_genitive(actors[i]): # check if token is a genitive phrase\n",
    "        \n",
    "        pos_index = len(extag)\n",
    "        extag += actors[i][0]\n",
    "        if prev_f: \n",
    "            continue\n",
    "            \n",
    "    # check if preposition non-keyword preposition followers an actor\n",
    "    elif is_actor(actors[i][1]) and prev_p:\n",
    "        \n",
    "        extag = extag[:pos_index] + actors[i][0] + extag[pos_index:] # switch order of actor and preposition\n",
    "        prev_p = False # set current parsing of preposition to false\n",
    "        \n",
    "    elif is_prep(actors[i][1]) or actors[i][0].lower() in config.keywords: # check if token is a verb phrase\n",
    "        \n",
    "        if open_v: \n",
    "            extag += ')' # close open verb tag\n",
    "            open_v = False\n",
    "        \n",
    "        # substitute with keyword representation encoded in domain knowledge\n",
    "        op = config.keywords[actors[i][0].lower()]\n",
    "        \n",
    "        if not op:\n",
    "            op = '=>*(%' + actors[i][0].lower() + '%)' # substitute with user-specified token if not in keywords\n",
    "            \n",
    "        if op in config.filters:\n",
    "            \n",
    "            if flag == 1:\n",
    "                \n",
    "                # concatenate keyword representation to extag\n",
    "                extag = extag[:pos_index] + op + '(' + extag[pos_index:]\n",
    "                pos_index += len(op) + 1\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                pos_index = len(extag)\n",
    "                extag += op + '('\n",
    "                \n",
    "            prev_p, prev_f = False, True # not checking a prepositition but are checking a filter\n",
    "            continue\n",
    "            \n",
    "        extag = extag[:pos_index] + op + extag[pos_index:] # update result\n",
    "        prev_p = True # set parsing of preposition to true\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if flag == 1: # flag marks prepositional clauses succeeding a verb phrase that it modifies\n",
    "            extag = extag[:pos_index] + actors[i][0] + ')' + extag[pos_index:]\n",
    "            prev_f = False\n",
    "            \n",
    "        pos_index = len(extag)\n",
    "        \n",
    "        if flag == 1:\n",
    "            \n",
    "            flag = -1\n",
    "            \n",
    "            if (i+1) < len(actors) and is_actor(actors[i+1][1]):\n",
    "                extag += '=>*(%filter%)(' # update result to accommodate parsed actions that require a parameter\n",
    "                prev_f = True # set parsing of filter to true\n",
    "            continue\n",
    "            \n",
    "        extag += actors[i][0] # update result\n",
    "        \n",
    "        if prev_f and (i+1) < len(actors) and actors[i+1][0].lower() == 'of':\n",
    "            \n",
    "            flag = 0\n",
    "            extag += ')'\n",
    "            pos_index = len(extag)\n",
    "            continue\n",
    "            \n",
    "    if (i+1) < len(actors) and is_conj(actors[i+1][0]): # ignore conjunctions that were handled above\n",
    "        continue\n",
    "        \n",
    "    if prev_f: # check if modifiying a filtering substitution\n",
    "        \n",
    "        pos_index = len(extag)\n",
    "        \n",
    "        if ((i+1) < len(actors) and \n",
    "            (is_genitive(actors[i+1]) or is_actor(actors[i+1][1]))):\n",
    "            \n",
    "            if is_actor(actors[i+1][1]):\n",
    "                extag += ')=>*(%filter%)(' # accommodate action that requires a parameter\n",
    "            continue\n",
    "            \n",
    "        extag += ')'\n",
    "        \n",
    "    prev_f = False\n",
    "    \n",
    "if open_v or prev_f:\n",
    "    extag += ')'\n",
    "    \n",
    "for s in config.subs: # substitute tokens in the parsed result that match tokens in subs\n",
    "    extag = extag.replace(s, config.subs[s])\n",
    "    \n",
    "extag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered: \t['=>*(%filter%)((oriole pitchers))', '=>*(%filter%)((fastballs)OR(curveballs))', '=>*(%filter%)((start velocity))', '=>*(%over%)((90))', '=>*(%filter%)((pitch))', '=>*(%filter%)((horizontal movement))', '=>*(%under%)((12))', '=>*(%filter%)((pitch))', '=>*(%filter%)((height))', '=>*(%over%)((2))']\n",
      "entity: \t['(lefty batters)', '->', '(on-base percentage)']\n"
     ]
    }
   ],
   "source": [
    "f_list, e_list = [], [] # initialize filters, entities lists\n",
    "f_keys = [re.findall(r'\\(%(.+?)%\\)', f_key)[0] for f_key in config.filters] # extract only content\n",
    "f_keys = '(?:' + '|'.join(f_keys) + ')'\n",
    "pat = r'=>\\*\\(' + '%' + f_keys + '%' + r'\\)\\(\\(.+?\\)\\)\\)?' # pattern match against any filter in domain knowledge\n",
    "pat_e = r'(' + pat + ')'\n",
    "f_list.append(re.findall(pat, extag))\n",
    "\n",
    "e_list = re.split(pat_e, extag)\n",
    "f_list = list(itertools.chain.from_iterable(f_list))\n",
    "e_list = [expr for expr in e_list if expr and re.search(pat_e, expr) is None]\n",
    "e_list = [ent for expr in e_list for ent in re.split(r'(\\(.+?\\)\\)?)', expr) if ent] # holds non-filter entities\n",
    "pos_s, pos_e = -1, -1\n",
    "\n",
    "for i in range(len(e_list)):\n",
    "    \n",
    "    if e_list[i] == '=>*': # mark actions\n",
    "        pos_s = i    \n",
    "    elif re.search(r'\\(\\(.+\\)\\)', e_list[i]):\n",
    "        pos_e = i    \n",
    "    if pos_s > 0 and pos_e > 0:\n",
    "        e_list[pos_s:(pos_e+1)] = [''.join(e_list[pos_s:(pos_e+1)])]\n",
    "        pos_s, pos_e = -1, -1\n",
    "        \n",
    "print('filtered: ', end='\\t'); print(f_list)\n",
    "print('entity: ', end='\\t'); print(e_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature association: most relevant feature is team_id_pitcher\n",
      "(team_id_pitcher, oriole pitchers): match_arg_to_feature_value\n",
      "generate_criteria: ['balmlb']\n",
      "\n",
      "feature association: most relevant feature is pitch_type\n",
      "(pitch_type, fastballs): match_arg_to_feature_value\n",
      "generate_criteria: ['FA', 'FF', 'FT', 'FC', 'FS']\n",
      "(pitch_type, curveballs): match_arg_to_feature_value\n",
      "generate_criteria: ['CB', 'CU']\n",
      "\n",
      "feature association: most relevant feature is start_speed\n",
      "\n",
      "feature association: most relevant feature is pfx_x\n",
      "\n",
      "feature association: most relevant feature is pz\n",
      "(team_id_pitcher, ['balmlb']): filter_\n",
      "\n",
      "config.filtered = config.filtered[(config.filtered['team_id_pitcher'] == 'balmlb')]\n",
      "(pitch_type, ['FA+FF+FT+FC+FS+CB+CU']): filter_\n",
      "\n",
      "config.filtered = config.filtered[(config.filtered['pitch_type'] == 'FA') | (config.filtered['pitch_type'] == 'FF') | (config.filtered['pitch_type'] == 'FT') | (config.filtered['pitch_type'] == 'FC') | (config.filtered['pitch_type'] == 'FS') | (config.filtered['pitch_type'] == 'CB') | (config.filtered['pitch_type'] == 'CU')]\n",
      "\n",
      "config.filtered = config.filtered[(config.filtered['start_speed'] > 90)]\n",
      "\n",
      "config.filtered = config.filtered[(config.filtered['pfx_x'] < 12)]\n",
      "\n",
      "config.filtered = config.filtered[(config.filtered['pz'] > 2)]\n",
      "(stand, lefty): match_arg_to_feature_value\n",
      "(stand, batters): match_arg_to_feature_value\n",
      "(stand, ['L']): filter_\n",
      "\n",
      "config.filtered = config.filtered[(config.filtered['stand'] == 'L')]\n",
      "(stand, ['L+R']): filter_\n",
      "\n",
      "config.filtered = config.filtered[(config.filtered['stand'] == 'L') | (config.filtered['stand'] == 'R')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**\n",
       "Oracle's response: 0.3385918784691791**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**\n",
       "(Relevance Feedback) Investigate Features More Like This: ['pz', 'pfx_x', 'start_speed', 'pitch_type', 'team_id_pitcher']**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~smohan/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.filtered, b_feat = sequential_filter(f_list) # apply sequential filter\n",
    "sequential_entity(e_list, b_feat, actors) # apply sequential entity filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_RankSVM(plotDF):\n",
    "    \n",
    "    '''Train a RankSVM to learn orderings between automated visualizations on the basis\n",
    "    of contextual relevance (query specific) and plot aesthetics.\n",
    "    \n",
    "    In the ranking setting, training data constructed from a list of items \n",
    "    are constructed with an order specified between items in each list. \n",
    "    \n",
    "    The order is induced by providing either a numerical/ordinal score or relevant/nonrelevant\n",
    "    judgement to each item in the list.\n",
    "    \n",
    "    Results evaluated by Kendall Tao correlation coefficient when provided list of \n",
    "    relevant/nonrelevant visualizations.\n",
    "    '''\n",
    "    \n",
    "    from sklearn import cross_validation\n",
    "    \n",
    "    X = plotDF.as_matrix() # convert pandas dataframe to numpy matrix representation\n",
    "    y = plotDF[:,-1] # class labels\n",
    "    X = X[:,:-1] # feature matrix\n",
    "    cv = cross_validation.StratifiedShuffleSplit(y, test_size=0.2) # cross validated shuffle split\n",
    "    train, test = next(iter(cv)) # construct train/test partitions\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    comb = itertools.combinations(range(X_train.shape[0]), 2)\n",
    "    k = 0\n",
    "    Xp, yp, diff = [], [], []\n",
    "    \n",
    "    for (i, j) in comb:\n",
    "        \n",
    "        if y_train[i] == y_train[j]:\n",
    "            # skip if same target or different group\n",
    "            continue\n",
    "        \n",
    "        Xp.append(X_train[i] - X_train[j]) # compute pairwise differences across training set\n",
    "        diff.append(y_train[i] - y_train[j]) # append to diff list\n",
    "        yp.append(np.sign(diff[-1]))\n",
    "        \n",
    "        # output balanced classes\n",
    "        if yp[-1] != (-1) ** k:\n",
    "            yp[-1] *= -1\n",
    "            Xp[-1] *= -1\n",
    "            diff[-1] *= -1\n",
    "        k += 1\n",
    "        \n",
    "    Xp, yp, diff = map(np.asanyarray, (Xp, yp, diff))\n",
    "    clf = svm.SVC(kernel='linear', C=.1) # construct linear SVM to generated classify pairwise orderings\n",
    "    clf.fit(Xp, yp) # fit RankSVM\n",
    "    coef = clf.coef_.ravel() / np.linalg.norm(clf.coef_)\n",
    "    tau, _ = scipy.stats.kendalltau(np.dot(X_test, coef), y_test) # evaluate by Kendall Tao correlation\n",
    "    print('Kendall correlation coefficient: %.5f' % (tau))\n",
    "    \n",
    "    model_filename = os.path.join(os.path.expanduser('~'), 'Jupyter', 'Extract', 'ranksvm.pkl')\n",
    "    joblib.dump(clf, model_filename) # save model\n",
    "    \n",
    "def test_RankSVM(plotDF):\n",
    "    \n",
    "    '''Return index orderings of automated visualizations encoded into plotDF'''\n",
    "    \n",
    "    X = plotDF.as_matrix() # convert plotDF from pandas dataframe to numpy matrix\n",
    "    model_filename = os.path.join(os.path.expanduser('~'), 'Jupyter', 'Extract', 'ranksvm.pkl')\n",
    "    clf = joblib.load(model_filename) # load saved model\n",
    "    coef = clf.coef_.ravel() / np.linalg.norm(clf.coef_) # apply trained RankSVM to predict ranking order\n",
    "    result = np.dot(X, coef)\n",
    "    result = [i[0] for i in sorted(enumerate(result), key=lambda x:x[1])]\n",
    "    \n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
